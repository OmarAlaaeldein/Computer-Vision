{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "Dtrb-zHT4lJp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04vtCv7_NFnQ"
      },
      "source": [
        "## Problem 1. Neural Net from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG1tEQn66VdL"
      },
      "source": [
        "### Dense Layer (Feed Forward Layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "Ig2f7mXu4rsP"
      },
      "outputs": [],
      "source": [
        "class Dense():\n",
        "    def __init__(self, in_dim, out_dim, bias_flag=True):\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.bias_flag = bias_flag\n",
        "        self.test = False\n",
        "\n",
        "        self.weight = np.random.rand(in_dim, out_dim)\n",
        "        self.bias = np.random.rand(1, out_dim)\n",
        "\n",
        "        self.weight_grad = np.zeros((in_dim, out_dim))\n",
        "        self.bias_grad = np.zeros((1, out_dim))\n",
        "\n",
        "    def __call__(self, input, test=False):\n",
        "        assert isinstance(input, np.ndarray), 'Input must be an array'\n",
        "        #assert input.shape[\n",
        "        #    1] == self.in_dim, 'Dimensions mismatch {0} != {1}'.format(\n",
        "         #       input.shape[1], self.in_dim)\n",
        "\n",
        "        self.input = input\n",
        "        return self.forward(input)\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weight.fill(0.01)\n",
        "\n",
        "    def init_bias(self):\n",
        "        self.bias.fill(0.01)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        self.batch_size = input.shape[0]\n",
        "        ss=[]\n",
        "        if not self.bias_flag:\n",
        "            for i in range(self.batch_size):\n",
        "                \n",
        "                print(self.weight.shape)\n",
        "                ss.append(np.dot(self.weight.T,self.input[i]))\n",
        "            return ss\n",
        "        else:\n",
        "            for i in range(self.batch_size):\n",
        "                #print(input[i].shape)\n",
        "                #print(self.weight.T.shape)\n",
        "                #print(self.bias.shape)\n",
        "                \n",
        "                ss.append(np.dot(np.atleast_2d(self.input[i]),self.weight)+self.bias)\n",
        "                # print('a'*5)\n",
        "\n",
        "            return ss\n",
        "\n",
        "    def backward(self, backprop_err):\n",
        "        for i in range(self.batch_size):\n",
        "            #print(self.input[i].shape,'inputshape')\n",
        "            #print(self.weight_grad[i].shape,'grad_i')\n",
        "            #print(backprop_err[i].shape,'backproperr_i')\n",
        "            #print(backprop_err,'backproperr_itself')\n",
        "           # print(backprop_err[i],'backproperr_itself_i')\n",
        "            try:\n",
        "                self.weight_grad += np.dot(np.atleast_2d(self.input[i]).T,backprop_err[i][0].reshape(-1,1).T)/self.batch_size\n",
        "            except ValueError as e:\n",
        "                #print(e)\n",
        "                self.weight_grad += np.dot(np.atleast_2d(self.input[i]),backprop_err[i][0].reshape(-1,1).T)/self.batch_size\n",
        "            #print(self.bias_grad)\n",
        "            self.bias_grad += np.sum(backprop_err[i][0])/self.batch_size\n",
        "        return backprop_err, self.weight\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.weight += lr * self.weight_grad\n",
        "        self.bias += lr * self.bias_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBwO_oMD6RQb"
      },
      "source": [
        "### Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "0emgcAjU532r"
      },
      "outputs": [],
      "source": [
        "class Sigmoid():\n",
        "    def __init__(self, test=False):\n",
        "        self.grad = None\n",
        "        self.test= test\n",
        "\n",
        "    def __call__(self, input, test=None):\n",
        "        out = 1 / (1 + np.exp(-1 * input))\n",
        "        if test is None: test = self.test\n",
        "        if not test:\n",
        "            self.grad = out * (1 - out)\n",
        "        return out\n",
        "    \n",
        "    def backward(self, backprop_err, weight=None):\n",
        "        if weight is None:\n",
        "            return backprop_err * self.grad\n",
        "        else:\n",
        "            return (backprop_err @ weight.T) * self.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "KVW-lwqS8W7T"
      },
      "outputs": [],
      "source": [
        "class Linear():\n",
        "    def __init__(self, test=False):\n",
        "        self.grad = None\n",
        "        self.test= test\n",
        "\n",
        "    def __call__(self, input, test=None):\n",
        "        out = input\n",
        "        if test is None: test = self.test\n",
        "        if not test:\n",
        "            self.grad = np.ones_like(out)\n",
        "        return out\n",
        "    \n",
        "    def backward(self, backprop_err, weight=None):\n",
        "        if weight is None:\n",
        "            return backprop_err * self.grad\n",
        "        else:\n",
        "            return (backprop_err @ weight.T) * self.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "u7kAT74g8jA5"
      },
      "outputs": [],
      "source": [
        "class Relu():\n",
        "    def __init__(self, test=False):\n",
        "        self.grad = None\n",
        "        self.test= test\n",
        "\n",
        "    def __call__(self, input, test=None):\n",
        "        out = np.maximum(input,0)\n",
        "        if test is None: test = self.test\n",
        "        if not test:\n",
        "            self.grad = np.greater(out,0)*1\n",
        "        return out\n",
        "    \n",
        "    def backward(self, backprop_err, weight=None):\n",
        "        if weight is None:\n",
        "            return backprop_err * self.grad\n",
        "        else:\n",
        "            return (backprop_err @ weight.T) * self.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "d_02o-Mt8oQl"
      },
      "outputs": [],
      "source": [
        "class Tanh():\n",
        "    def __init__(self, test=False):\n",
        "        self.grad = None\n",
        "        self.test= test\n",
        "    \n",
        "    def __call__(self, input, test=None):\n",
        "        out = (np.exp(input)-np.exp(-input))/(np.exp(input)+np.exp(-input))\n",
        "        if test is None: test = self.test\n",
        "        if not test:\n",
        "            self.grad = 1-(np.exp(input)-np.exp(-input))/(np.exp(input)+np.exp(-input))**2\n",
        "        return out\n",
        "    \n",
        "    def backward(self, backprop_err, weight=None):\n",
        "        if weight is None:\n",
        "            return backprop_err * self.grad\n",
        "        else:\n",
        "            return (backprop_err @ weight.T) * self.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jodQlkiO8u3A"
      },
      "source": [
        "### Loss Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "PvI5aJRK8uC1"
      },
      "outputs": [],
      "source": [
        "class MSE():\n",
        "    def __init__(self):\n",
        "        self.grad = None\n",
        "\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        error = np.sum(np.sum((y_pred - y_true)**2, axis=0))\n",
        "        self.grad = 2*(y_pred - y_true)\n",
        "        return error\n",
        "\n",
        "    def backward(self):\n",
        "        return self.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va2vEplcB4aG"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "Ig7SYZGHBeyC"
      },
      "outputs": [],
      "source": [
        "class SequentialModel:\n",
        "    def __init__(self, *layers):\n",
        "        self.layers = []\n",
        "        self.updatable = []\n",
        "\n",
        "        for layer in layers:\n",
        "            self.layers.append(layer)\n",
        "            if hasattr(layer, 'update'):\n",
        "                if callable(layer.update):\n",
        "                    layer.init_weight()\n",
        "                    layer.init_bias()\n",
        "                    self.updatable.append(layer)\n",
        "\n",
        "    def __call__(self, input, test=None):\n",
        "        forwarded = input\n",
        "        for layer in self.layers:\n",
        "            forwarded = layer(forwarded, test)\n",
        "        return forwarded\n",
        "\n",
        "    def backward(self, criterion):\n",
        "        backwarded = criterion.backward()\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            #print(layer)\n",
        "            if isinstance(backwarded, tuple):\n",
        "                backwarded = layer.backward(*backwarded)\n",
        "            else:\n",
        "                backwarded = layer.backward(backwarded)\n",
        "    \n",
        "    def update(self, lr):\n",
        "        for layer in self.updatable:\n",
        "            layer.update(lr)\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.seq = SequentialModel(Dense(2, 4), Relu(),\n",
        "        Dense(4, 4),\n",
        "        Relu(),\n",
        "        Dense(4, 1),\n",
        "        Relu())\n",
        "\n",
        "    def __call__(self, input, test=None):\n",
        "        return self.forward(input, test)\n",
        "\n",
        "    def forward(self, input, test):      \n",
        "        self.y_pred = self.seq(input)\n",
        "        return self.y_pred\n",
        "\n",
        "    def backward(self):\n",
        "        self.seq.backward(self.criterion)\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.seq.update(lr)\n",
        "            \n",
        "    def loss(self, criterion, y_true):\n",
        "        self.criterion = criterion\n",
        "        return self.criterion(self.y_pred, y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZOOCoeNIOA9",
        "outputId": "655fceb4-f2bb-4f35-867a-01f8d36977d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "(3, 1)\n",
            "(3, 2)\n",
            "model output: [[0.010464]] [[0.0105008]] [[0.01048]] [[0.010472]] [[0.0105008]] [[0.01048]]\n",
            "***** Loss: 79.31389397145365\n",
            "Epoch: 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "(3, 1)\n",
            "(3, 2)\n",
            "model output: [[0.76107802]] [[0.76115529]] [[0.76111164]] [[0.76109482]] [[0.76115529]] [[0.76111164]]\n",
            "***** Loss: 64.60700645570695\n",
            "Epoch: 2 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "(3, 1)\n",
            "(3, 2)\n",
            "model output: [[2.12867001]] [[2.1289436]] [[2.12878919]] [[2.12872951]] [[2.1289436]] [[2.12878919]]\n",
            "***** Loss: 42.15617835180655\n",
            "Epoch: 3 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "(3, 1)\n",
            "(3, 2)\n",
            "model output: [[3.90625209]] [[3.90722429]] [[3.90667635]] [[3.90646362]] [[3.90722429]] [[3.90667635]]\n",
            "***** Loss: 21.36020544455073\n",
            "Epoch: 4 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "(3, 1)\n",
            "(3, 2)\n",
            "model output: [[5.71166357]] [[5.71363027]] [[5.71252276]] [[5.71209159]] [[5.71363027]] [[5.71252276]]\n",
            "***** Loss: 9.945221283298588\n",
            "Epoch: 5 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "(3, 1)\n",
            "(3, 2)\n",
            "model output: [[6.74591895]] [[6.74632112]] [[6.74609414]] [[6.74600642]] [[6.74632112]] [[6.74609414]]\n",
            "***** Loss: 7.814459554218472\n",
            "Epoch: 6 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "(3, 1)\n",
            "(3, 2)\n",
            "model output: [[7.30490637]] [[7.30490637]] [[7.30490637]] [[7.30490637]] [[7.30490637]] [[7.30490637]]\n",
            "***** Loss: 7.996928438792909\n"
          ]
        }
      ],
      "source": [
        "model = Model()\n",
        "mean_square_err = MSE()\n",
        "\n",
        "for epoch in range(7):\n",
        "    print(\"Epoch:\", epoch, \"~\"*50)\n",
        "    input, target = np.array([[1, 2], [4, 1.3], [1, 3],[1.5, 2], [4, 1.3], [1, 3]]), np.array([[3.75], [9],[8]])\n",
        "    print(np.array([[4], [9],[8]]).shape)\n",
        "    print(np.array([[1, 2], [4, 1.3], [1, 3]]).shape)\n",
        "    batch_size = input.shape[0]*2\n",
        "    out = model(input)\n",
        "    print(\"model output:\", *out)\n",
        "    loss = model.loss(mean_square_err, target) / batch_size\n",
        "    print(\"*\"*5, \"Loss:\", loss)\n",
        "    model.backward()\n",
        "    model.update(lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHlfgjEYAJrz"
      },
      "source": [
        "### *Testing Our Model Against Pytorch Model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "ZEbzoVjsARtZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)\n",
        "        self.fc2 = nn.Linear(4, 4)\n",
        "        self.fc3 = nn.Linear(4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.fc1.weight.data.fill_(0.01)\n",
        "        self.fc1.bias.data.fill_(0.01)\n",
        "        self.fc2.weight.data.fill_(0.01)\n",
        "        self.fc2.bias.data.fill_(0.01)\n",
        "        self.fc3.weight.data.fill_(0.01)\n",
        "        self.fc3.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "Q-q2K6sfBkN5"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0, dampening=0, weight_decay=0, nesterov=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "UFMZEwurDrq6"
      },
      "outputs": [],
      "source": [
        "net.init_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr5PHhSCBrnI",
        "outputId": "58f7f4a3-9326-4466-b690-8f86ec73056e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([0.0105]) tensor([0.0105])\n",
            "***** Loss: 48.363746643066406\n",
            "Epoch: 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([1.3105]) tensor([1.3106])\n",
            "***** Loss: 33.18022155761719\n",
            "Epoch: 2 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([2.3568]) tensor([2.3572])\n",
            "***** Loss: 23.413333892822266\n",
            "Epoch: 3 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([3.2124]) tensor([3.2138])\n",
            "***** Loss: 17.049976348876953\n",
            "Epoch: 4 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([3.9408]) tensor([3.9468])\n",
            "***** Loss: 12.769041061401367\n",
            "Epoch: 5 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([4.6128]) tensor([4.6426])\n",
            "***** Loss: 9.681167602539062\n",
            "Epoch: 6 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([5.3309]) tensor([5.4963])\n",
            "***** Loss: 7.023585319519043\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(7):\n",
        "    print(\"Epoch:\", epoch, \"~\"*50)\n",
        "    inputs, labels = torch.tensor([[1.0, 2.0], [4.0, 1.3]]), torch.tensor([[4.0], [9.0]])\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(inputs)\n",
        "    print(\"model output:\", *outputs.data)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"*\"*5, \"Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHgBoUbYObS4"
      },
      "source": [
        "## Problem 3. Iris Dataset Classification using Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "RwSgInkdOmwj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([150, 4])\n",
            "torch.Size([150, 1])\n",
            "Epoch: 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0107]) tensor([0.0107]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0106]) tensor([0.0107]) tensor([0.0106]) tensor([0.0106]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0106]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107]) tensor([0.0107])\n",
            "***** Loss: 1.6454297304153442\n",
            "Epoch: 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([0.2088]) tensor([0.2088]) tensor([0.2087]) tensor([0.2087]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2087]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2087]) tensor([0.2087]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2087]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2087]) tensor([0.2088]) tensor([0.2088]) tensor([0.2087]) tensor([0.2087]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2087]) tensor([0.2088]) tensor([0.2088]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2088]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2088]) tensor([0.2089]) tensor([0.2088]) tensor([0.2088]) tensor([0.2089]) tensor([0.2088]) tensor([0.2089]) tensor([0.2088]) tensor([0.2089]) tensor([0.2089]) tensor([0.2088]) tensor([0.2089]) tensor([0.2088]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2088]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2088]) tensor([0.2088]) tensor([0.2089]) tensor([0.2088]) tensor([0.2088]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2088]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2090]) tensor([0.2088]) tensor([0.2090]) tensor([0.2089]) tensor([0.2090]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2090]) tensor([0.2090]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2090]) tensor([0.2089]) tensor([0.2089]) tensor([0.2090]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2090]) tensor([0.2090]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2090]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2090]) tensor([0.2090]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089]) tensor([0.2089])\n",
            "***** Loss: 1.2924754619598389\n",
            "Epoch: 2 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([0.3674]) tensor([0.3673]) tensor([0.3673]) tensor([0.3673]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3673]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3673]) tensor([0.3673]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3673]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3673]) tensor([0.3674]) tensor([0.3674]) tensor([0.3673]) tensor([0.3673]) tensor([0.3674]) tensor([0.3674]) tensor([0.3673]) tensor([0.3674]) tensor([0.3673]) tensor([0.3674]) tensor([0.3674]) tensor([0.3676]) tensor([0.3675]) tensor([0.3676]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3674]) tensor([0.3675]) tensor([0.3675]) tensor([0.3674]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3676]) tensor([0.3675]) tensor([0.3674]) tensor([0.3674]) tensor([0.3674]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3674]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3675]) tensor([0.3674]) tensor([0.3675]) tensor([0.3676]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3675]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3677]) tensor([0.3677]) tensor([0.3675]) tensor([0.3676]) tensor([0.3675]) tensor([0.3676]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3675]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3677]) tensor([0.3676]) tensor([0.3675]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3676]) tensor([0.3675]) tensor([0.3676]) tensor([0.3676]) tensor([0.3675])\n",
            "***** Loss: 1.0666042566299438\n",
            "Epoch: 3 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4942]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4942]) tensor([0.4943]) tensor([0.4944]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4942]) tensor([0.4943]) tensor([0.4943]) tensor([0.4942]) tensor([0.4942]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4943]) tensor([0.4945]) tensor([0.4945]) tensor([0.4945]) tensor([0.4944]) tensor([0.4945]) tensor([0.4944]) tensor([0.4945]) tensor([0.4943]) tensor([0.4945]) tensor([0.4944]) tensor([0.4943]) tensor([0.4945]) tensor([0.4944]) tensor([0.4945]) tensor([0.4944]) tensor([0.4945]) tensor([0.4945]) tensor([0.4944]) tensor([0.4945]) tensor([0.4944]) tensor([0.4945]) tensor([0.4944]) tensor([0.4945]) tensor([0.4945]) tensor([0.4945]) tensor([0.4945]) tensor([0.4945]) tensor([0.4945]) tensor([0.4945]) tensor([0.4944]) tensor([0.4944]) tensor([0.4944]) tensor([0.4944]) tensor([0.4945]) tensor([0.4944]) tensor([0.4945]) tensor([0.4945]) tensor([0.4944]) tensor([0.4944]) tensor([0.4944]) tensor([0.4944]) tensor([0.4945]) tensor([0.4944]) tensor([0.4943]) tensor([0.4944]) tensor([0.4944]) tensor([0.4944]) tensor([0.4945]) tensor([0.4943]) tensor([0.4944]) tensor([0.4946]) tensor([0.4945]) tensor([0.4946]) tensor([0.4945]) tensor([0.4946]) tensor([0.4946]) tensor([0.4944]) tensor([0.4946]) tensor([0.4945]) tensor([0.4946]) tensor([0.4945]) tensor([0.4945]) tensor([0.4946]) tensor([0.4945]) tensor([0.4945]) tensor([0.4946]) tensor([0.4945]) tensor([0.4947]) tensor([0.4947]) tensor([0.4945]) tensor([0.4946]) tensor([0.4945]) tensor([0.4946]) tensor([0.4945]) tensor([0.4946]) tensor([0.4946]) tensor([0.4945]) tensor([0.4945]) tensor([0.4945]) tensor([0.4946]) tensor([0.4946]) tensor([0.4947]) tensor([0.4946]) tensor([0.4945]) tensor([0.4945]) tensor([0.4946]) tensor([0.4946]) tensor([0.4945]) tensor([0.4945]) tensor([0.4946]) tensor([0.4946]) tensor([0.4946]) tensor([0.4945]) tensor([0.4946]) tensor([0.4946]) tensor([0.4946]) tensor([0.4945]) tensor([0.4945]) tensor([0.4946]) tensor([0.4945])\n",
            "***** Loss: 0.9220870137214661\n",
            "Epoch: 4 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5959]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5959]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5959]) tensor([0.5959]) tensor([0.5959]) tensor([0.5958]) tensor([0.5959]) tensor([0.5959]) tensor([0.5959]) tensor([0.5959]) tensor([0.5958]) tensor([0.5959]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5959]) tensor([0.5959]) tensor([0.5959]) tensor([0.5958]) tensor([0.5958]) tensor([0.5959]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5958]) tensor([0.5959]) tensor([0.5959]) tensor([0.5958]) tensor([0.5959]) tensor([0.5958]) tensor([0.5959]) tensor([0.5958]) tensor([0.5961]) tensor([0.5961]) tensor([0.5961]) tensor([0.5960]) tensor([0.5961]) tensor([0.5960]) tensor([0.5961]) tensor([0.5959]) tensor([0.5961]) tensor([0.5960]) tensor([0.5959]) tensor([0.5960]) tensor([0.5960]) tensor([0.5961]) tensor([0.5960]) tensor([0.5961]) tensor([0.5960]) tensor([0.5960]) tensor([0.5960]) tensor([0.5960]) tensor([0.5961]) tensor([0.5960]) tensor([0.5961]) tensor([0.5961]) tensor([0.5961]) tensor([0.5961]) tensor([0.5961]) tensor([0.5961]) tensor([0.5961]) tensor([0.5960]) tensor([0.5960]) tensor([0.5960]) tensor([0.5960]) tensor([0.5961]) tensor([0.5960]) tensor([0.5961]) tensor([0.5961]) tensor([0.5960]) tensor([0.5960]) tensor([0.5960]) tensor([0.5960]) tensor([0.5961]) tensor([0.5960]) tensor([0.5959]) tensor([0.5960]) tensor([0.5960]) tensor([0.5960]) tensor([0.5961]) tensor([0.5959]) tensor([0.5960]) tensor([0.5962]) tensor([0.5961]) tensor([0.5962]) tensor([0.5961]) tensor([0.5962]) tensor([0.5963]) tensor([0.5960]) tensor([0.5962]) tensor([0.5962]) tensor([0.5963]) tensor([0.5962]) tensor([0.5961]) tensor([0.5962]) tensor([0.5961]) tensor([0.5961]) tensor([0.5962]) tensor([0.5962]) tensor([0.5963]) tensor([0.5963]) tensor([0.5961]) tensor([0.5962]) tensor([0.5961]) tensor([0.5963]) tensor([0.5961]) tensor([0.5962]) tensor([0.5962]) tensor([0.5961]) tensor([0.5961]) tensor([0.5962]) tensor([0.5962]) tensor([0.5962]) tensor([0.5963]) tensor([0.5962]) tensor([0.5961]) tensor([0.5961]) tensor([0.5963]) tensor([0.5962]) tensor([0.5962]) tensor([0.5961]) tensor([0.5962]) tensor([0.5962]) tensor([0.5962]) tensor([0.5961]) tensor([0.5962]) tensor([0.5962]) tensor([0.5962]) tensor([0.5961]) tensor([0.5961]) tensor([0.5962]) tensor([0.5961])\n",
            "***** Loss: 0.8296476006507874\n",
            "Epoch: 5 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([0.6771]) tensor([0.6770]) tensor([0.6770]) tensor([0.6770]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6770]) tensor([0.6770]) tensor([0.6771]) tensor([0.6771]) tensor([0.6770]) tensor([0.6770]) tensor([0.6771]) tensor([0.6772]) tensor([0.6771]) tensor([0.6771]) tensor([0.6772]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6770]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6771]) tensor([0.6770]) tensor([0.6771]) tensor([0.6771]) tensor([0.6770]) tensor([0.6771]) tensor([0.6771]) tensor([0.6770]) tensor([0.6770]) tensor([0.6771]) tensor([0.6771]) tensor([0.6770]) tensor([0.6771]) tensor([0.6770]) tensor([0.6771]) tensor([0.6771]) tensor([0.6774]) tensor([0.6774]) tensor([0.6774]) tensor([0.6773]) tensor([0.6774]) tensor([0.6773]) tensor([0.6774]) tensor([0.6772]) tensor([0.6774]) tensor([0.6773]) tensor([0.6772]) tensor([0.6773]) tensor([0.6773]) tensor([0.6774]) tensor([0.6773]) tensor([0.6774]) tensor([0.6773]) tensor([0.6773]) tensor([0.6773]) tensor([0.6773]) tensor([0.6774]) tensor([0.6773]) tensor([0.6774]) tensor([0.6773]) tensor([0.6774]) tensor([0.6774]) tensor([0.6774]) tensor([0.6774]) tensor([0.6774]) tensor([0.6772]) tensor([0.6772]) tensor([0.6772]) tensor([0.6773]) tensor([0.6774]) tensor([0.6773]) tensor([0.6774]) tensor([0.6774]) tensor([0.6773]) tensor([0.6773]) tensor([0.6773]) tensor([0.6773]) tensor([0.6774]) tensor([0.6773]) tensor([0.6772]) tensor([0.6773]) tensor([0.6773]) tensor([0.6773]) tensor([0.6773]) tensor([0.6772]) tensor([0.6773]) tensor([0.6775]) tensor([0.6774]) tensor([0.6775]) tensor([0.6774]) tensor([0.6775]) tensor([0.6776]) tensor([0.6773]) tensor([0.6776]) tensor([0.6775]) tensor([0.6776]) tensor([0.6775]) tensor([0.6774]) tensor([0.6775]) tensor([0.6774]) tensor([0.6774]) tensor([0.6775]) tensor([0.6775]) tensor([0.6777]) tensor([0.6776]) tensor([0.6773]) tensor([0.6775]) tensor([0.6774]) tensor([0.6776]) tensor([0.6774]) tensor([0.6775]) tensor([0.6775]) tensor([0.6774]) tensor([0.6774]) tensor([0.6775]) tensor([0.6775]) tensor([0.6775]) tensor([0.6776]) tensor([0.6775]) tensor([0.6774]) tensor([0.6774]) tensor([0.6776]) tensor([0.6775]) tensor([0.6775]) tensor([0.6774]) tensor([0.6775]) tensor([0.6775]) tensor([0.6775]) tensor([0.6774]) tensor([0.6775]) tensor([0.6775]) tensor([0.6775]) tensor([0.6774]) tensor([0.6775]) tensor([0.6775]) tensor([0.6774])\n",
            "***** Loss: 0.7705397605895996\n",
            "Epoch: 6 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "model output: tensor([0.7421]) tensor([0.7420]) tensor([0.7420]) tensor([0.7420]) tensor([0.7421]) tensor([0.7421]) tensor([0.7420]) tensor([0.7421]) tensor([0.7420]) tensor([0.7420]) tensor([0.7421]) tensor([0.7420]) tensor([0.7420]) tensor([0.7419]) tensor([0.7421]) tensor([0.7422]) tensor([0.7421]) tensor([0.7421]) tensor([0.7421]) tensor([0.7421]) tensor([0.7421]) tensor([0.7421]) tensor([0.7420]) tensor([0.7421]) tensor([0.7421]) tensor([0.7420]) tensor([0.7421]) tensor([0.7421]) tensor([0.7421]) tensor([0.7420]) tensor([0.7420]) tensor([0.7421]) tensor([0.7421]) tensor([0.7421]) tensor([0.7420]) tensor([0.7420]) tensor([0.7421]) tensor([0.7420]) tensor([0.7420]) tensor([0.7421]) tensor([0.7421]) tensor([0.7419]) tensor([0.7420]) tensor([0.7421]) tensor([0.7421]) tensor([0.7420]) tensor([0.7421]) tensor([0.7420]) tensor([0.7421]) tensor([0.7420]) tensor([0.7425]) tensor([0.7424]) tensor([0.7425]) tensor([0.7423]) tensor([0.7424]) tensor([0.7423]) tensor([0.7424]) tensor([0.7422]) tensor([0.7424]) tensor([0.7423]) tensor([0.7422]) tensor([0.7424]) tensor([0.7423]) tensor([0.7424]) tensor([0.7423]) tensor([0.7424]) tensor([0.7424]) tensor([0.7423]) tensor([0.7423]) tensor([0.7423]) tensor([0.7424]) tensor([0.7423]) tensor([0.7424]) tensor([0.7424]) tensor([0.7424]) tensor([0.7424]) tensor([0.7424]) tensor([0.7425]) tensor([0.7424]) tensor([0.7422]) tensor([0.7422]) tensor([0.7422]) tensor([0.7423]) tensor([0.7424]) tensor([0.7423]) tensor([0.7424]) tensor([0.7425]) tensor([0.7423]) tensor([0.7423]) tensor([0.7423]) tensor([0.7423]) tensor([0.7424]) tensor([0.7423]) tensor([0.7422]) tensor([0.7423]) tensor([0.7423]) tensor([0.7423]) tensor([0.7424]) tensor([0.7422]) tensor([0.7423]) tensor([0.7426]) tensor([0.7424]) tensor([0.7426]) tensor([0.7425]) tensor([0.7425]) tensor([0.7427]) tensor([0.7423]) tensor([0.7426]) tensor([0.7425]) tensor([0.7427]) tensor([0.7425]) tensor([0.7425]) tensor([0.7425]) tensor([0.7424]) tensor([0.7425]) tensor([0.7425]) tensor([0.7425]) tensor([0.7427]) tensor([0.7427]) tensor([0.7424]) tensor([0.7426]) tensor([0.7424]) tensor([0.7427]) tensor([0.7424]) tensor([0.7426]) tensor([0.7426]) tensor([0.7424]) tensor([0.7424]) tensor([0.7425]) tensor([0.7426]) tensor([0.7426]) tensor([0.7427]) tensor([0.7425]) tensor([0.7424]) tensor([0.7424]) tensor([0.7427]) tensor([0.7426]) tensor([0.7425]) tensor([0.7424]) tensor([0.7426]) tensor([0.7426]) tensor([0.7425]) tensor([0.7424]) tensor([0.7426]) tensor([0.7426]) tensor([0.7425]) tensor([0.7424]) tensor([0.7425]) tensor([0.7425]) tensor([0.7424])\n",
            "***** Loss: 0.7327582240104675\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "data = torch.tensor(np.array(load_iris()['data']))\n",
        "target= torch.tensor(load_iris()['target']).reshape(-1,1)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(4, 4)\n",
        "        self.fc2 = nn.Linear(4, 4)\n",
        "        self.fc3 = nn.Linear(4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.fc1.weight.data.fill_(0.01)\n",
        "        self.fc1.bias.data.fill_(0.01)\n",
        "        self.fc2.weight.data.fill_(0.01)\n",
        "        self.fc2.bias.data.fill_(0.01)\n",
        "        self.fc3.weight.data.fill_(0.01)\n",
        "        self.fc3.bias.data.fill_(0.01)\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
        "net.init_weights()\n",
        "print(data.shape)\n",
        "print(target.shape)\n",
        "for epoch in range(7):\n",
        "    print(\"Epoch:\", epoch, \"~\"*50)\n",
        "    inputs, labels = data, target\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(inputs.float())\n",
        "    print(\"model output:\", *outputs.data)\n",
        "    loss = criterion(outputs.float(), labels.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"*\"*5, \"Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtjhLZdTRMnh"
      },
      "source": [
        "## Problem 4. Iris Dataset Classification using Neural Net with PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "etCIx1igRL-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 2)\n",
            "(150, 1)\n",
            "Epoch: 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 246.88908373957412\n",
            "Epoch: 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 246.269543003193\n",
            "Epoch: 2 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 244.91088828129563\n",
            "Epoch: 3 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 242.55386725986293\n",
            "Epoch: 4 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 238.75855484482133\n",
            "Epoch: 5 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 232.84348626179602\n",
            "Epoch: 6 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 223.81571232563715\n",
            "Epoch: 7 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 210.33392515952906\n",
            "Epoch: 8 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 190.83416095585488\n",
            "Epoch: 9 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "***** Loss: 164.17771374563992\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "data = load_iris()['data']\n",
        "target= load_iris()['target'].reshape(-1,1)\n",
        "sklearn_pca = PCA(n_components=2)\n",
        "data_new = sklearn_pca.fit_transform(data)\n",
        "print(data_new.shape)\n",
        "print(target.shape)\n",
        "model = Model()\n",
        "mean_square_err = MSE()\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(\"Epoch:\", epoch, \"~\"*50)\n",
        "    input, target = data_new, target\n",
        "    batch_size = input.shape[0]\n",
        "    out = model(input)\n",
        "    # print(\"model output:\", *out)\n",
        "    loss = model.loss(mean_square_err, target) / batch_size\n",
        "    print(\"*\"*5, \"Loss:\", loss)\n",
        "    model.backward()\n",
        "    model.update(lr=0.1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Omar Alaaeldein - CV_Lab_6.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a0219c7fb9a3ea5a3aef9cde8f981e20439ec62ccfaba430aba01f99d0659564"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
